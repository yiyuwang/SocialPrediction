{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# group level for Social Prediction\n",
    "\n",
    "*Yiyu Wang 2024 Feb*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yiyuwang/anaconda3/envs/neuroimaging_env/lib/python3.7/site-packages/nilearn/__init__.py:67: FutureWarning: Python 3.7 support is deprecated and will be removed in release 0.12 of Nilearn. Consider switching to Python 3.9 or 3.10.\n",
      "  _python_deprecation_warnings()\n",
      "/Users/yiyuwang/anaconda3/envs/neuroimaging_env/lib/python3.7/site-packages/nilearn/input_data/__init__.py:23: FutureWarning: The import path 'nilearn.input_data' is deprecated in version 0.9. Importing from 'nilearn.input_data' will be possible at least until release 0.13.0. Please import from 'nilearn.maskers' instead.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "import nilearn\n",
    "from nilearn.image import smooth_img, resample_to_img,new_img_like, math_img, concat_imgs, get_data\n",
    "from nilearn import image\n",
    "from nilearn import plotting\n",
    "from nilearn.masking import apply_mask\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn.glm.second_level import SecondLevelModel\n",
    "from nilearn.reporting import get_clusters_table\n",
    "from nilearn.glm import threshold_stats_img\n",
    "from scipy.stats import norm\n",
    "\n",
    "from nilearn.datasets import load_mni152_gm_mask,load_mni152_wm_mask,fetch_surf_fsaverage\n",
    "\n",
    "\n",
    "import gzip\n",
    "import math\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subjects in this analysis:\n",
      "[152, 179, 154, 158, 173, 153, 159, 174, 162, 145, 143, 181, 144, 169, 146, 167, 161, 182, 147, 166, 160, 185, 170, 176, 151, 157, 171, 177, 150, 156]\n",
      "**** n = 30 *****\n"
     ]
    }
   ],
   "source": [
    "# local directory set up\n",
    "gm_mask_img = nib.load('masks/gm_mask_icbm152_brain.nii.gz')\n",
    "\n",
    "subjects_list = pd.read_csv('Data/included_SocialPred_subjects.csv', header=None)\n",
    "subjects_list = subjects_list[0].values.tolist()\n",
    "sample_n = len(subjects_list)\n",
    "print(\"subjects in this analysis:\")\n",
    "print(subjects_list)\n",
    "print(f\"**** n = {sample_n} *****\" )\n",
    "\n",
    "\n",
    "TR = .001\n",
    "N_TR = 675\n",
    "TR_Length = 0.8\n",
    "TR_IN_MS = int(TR_Length/TR)\n",
    "\n",
    "fwhm = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "beta_dir = 'fmri_results/1stLvl/'\n",
    "second_level_res_dir = 'fmri_results/OneSampleT_3lvl/'\n",
    "\n",
    "if not os.path.isdir(second_level_res_dir):\n",
    "    os.mkdir(second_level_res_dir)\n",
    "\n",
    "\n",
    "vmax = 5\n",
    "cluster_thre = 30\n",
    "p_val = 0.05\n",
    "p_unc = norm.isf(p_val)\n",
    "\n",
    "p001 = 0.001\n",
    "p001_unc = norm.isf(p001)\n",
    "\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subject folders\n",
    "for s in subjects_list:\n",
    "    sub_output_dir = second_level_res_dir + f'/{s}/'\n",
    "    if not os.path.isdir(sub_output_dir):\n",
    "        os.makedirs(sub_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running subject 152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yiyuwang/anaconda3/envs/neuroimaging_env/lib/python3.7/site-packages/nilearn/glm/regression.py:207: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  self.whitened_design.shape[0] - self.whitened_design.shape[1]\n",
      "/Users/yiyuwang/anaconda3/envs/neuroimaging_env/lib/python3.7/site-packages/nilearn/glm/regression.py:207: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.whitened_design.shape[0] - self.whitened_design.shape[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running subject 179\n",
      "running subject 154\n",
      "running subject 158\n",
      "running subject 173\n",
      "running subject 153\n",
      "running subject 159\n",
      "running subject 174\n",
      "running subject 162\n",
      "running subject 145\n",
      "running subject 143\n",
      "running subject 181\n",
      "running subject 144\n",
      "running subject 169\n",
      "running subject 146\n",
      "running subject 167\n",
      "running subject 161\n",
      "running subject 182\n",
      "running subject 147\n",
      "running subject 166\n",
      "running subject 160\n",
      "running subject 185\n",
      "running subject 170\n",
      "running subject 176\n",
      "running subject 151\n",
      "running subject 157\n",
      "running subject 171\n",
      "running subject 177\n",
      "running subject 150\n",
      "running subject 156\n"
     ]
    }
   ],
   "source": [
    "# run obs social - obs pattern contrast:\n",
    "for s in subjects_list:\n",
    "    sub_output_dir = second_level_res_dir + f'/{s}/'\n",
    "    # run contrast:\n",
    "    print(f'running subject {s}')\n",
    "    second_level_input = []\n",
    "    obs_social_pattern_contrast = []\n",
    "\n",
    "    file_name = glob.glob(beta_dir + f'{s}/sub-{s}*z_score_*obs_Pattern*.nii.gz')\n",
    "    second_level_input.append(file_name[0])\n",
    "    obs_social_pattern_contrast.append(-1)\n",
    "\n",
    "\n",
    "    file_name = glob.glob(beta_dir + f'{s}/sub-{s}*z_score_*obs_Social*.nii.gz')\n",
    "    second_level_input.append(file_name[0])\n",
    "    obs_social_pattern_contrast.append(1)\n",
    "\n",
    "    design_columns = ['intercept','obs_social_pattern']\n",
    "\n",
    "    X = pd.concat([pd.Series(np.ones(len(second_level_input))), pd.Series(obs_social_pattern_contrast)], axis=1)\n",
    "    X.columns= design_columns\n",
    "\n",
    "\n",
    "    second_level_model = SecondLevelModel()\n",
    "    second_level_model = second_level_model.fit(second_level_input,\n",
    "                                                design_matrix=X)\n",
    "    \n",
    "    # get the beta:\n",
    "    stats_name = 'effect_size'\n",
    "    res = second_level_model.compute_contrast('obs_social_pattern',output_type=stats_name)\n",
    "    nii_file_path = sub_output_dir + f'/obs_social_pattern_{stats_name}.nii.gz'\n",
    "    nib.save(res, nii_file_path)\n",
    "\n",
    "\n",
    "    plotting.plot_design_matrix(X, output_file=join(second_level_res_dir, f'design_matrix.png'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running subject 152\n",
      "running subject 179\n",
      "running subject 154\n",
      "running subject 158\n",
      "running subject 173\n",
      "running subject 153\n",
      "running subject 159\n",
      "running subject 174\n",
      "running subject 162\n",
      "running subject 145\n",
      "running subject 143\n",
      "running subject 181\n",
      "running subject 144\n",
      "running subject 169\n",
      "running subject 146\n",
      "running subject 167\n",
      "running subject 161\n",
      "running subject 182\n",
      "running subject 147\n",
      "running subject 166\n",
      "running subject 160\n",
      "running subject 185\n",
      "running subject 170\n",
      "running subject 176\n",
      "running subject 151\n",
      "running subject 157\n",
      "running subject 171\n",
      "running subject 177\n",
      "running subject 150\n",
      "running subject 156\n"
     ]
    }
   ],
   "source": [
    "# run anova:\n",
    "\n",
    "for s in subjects_list:\n",
    "    sub_output_dir = second_level_res_dir + f'/{s}/'\n",
    "    # run contrast:\n",
    "    print(f'running subject {s}')\n",
    "    \n",
    "    file_list = []\n",
    "    main_condition_contrast, main_PE_contrast, interaction_contrast = [], [], []\n",
    "\n",
    "\n",
    "    file_name = glob.glob(beta_dir + f'{s}/sub-{s}*z_score_*fb_Social_PE*.nii.gz')\n",
    "    file_list.append(file_name[0])\n",
    "    main_condition_contrast.append(1)\n",
    "    main_PE_contrast.append(1) \n",
    "    interaction_contrast.append(1)\n",
    "\n",
    "    file_name = glob.glob(beta_dir + f'{s}/sub-{s}*z_score_*fb_Social_Congruent*.nii.gz')\n",
    "    file_list.append(file_name[0])\n",
    "    main_condition_contrast.append(1)\n",
    "    main_PE_contrast.append(-1) \n",
    "    interaction_contrast.append(-1)\n",
    "\n",
    "    file_name = glob.glob(beta_dir + f'{s}/sub-{s}*z_score_*fb_Pattern_PE*.nii.gz')\n",
    "    file_list.append(file_name[0])\n",
    "    main_condition_contrast.append(-1)\n",
    "    main_PE_contrast.append(1) \n",
    "    interaction_contrast.append(-1)\n",
    "\n",
    "    file_name = glob.glob(beta_dir + f'{s}/sub-{s}*z_score_*fb_Pattern_Congruent*.nii.gz')\n",
    "    file_list.append(file_name[0])\n",
    "    main_condition_contrast.append(-1)\n",
    "    main_PE_contrast.append(-1) \n",
    "    interaction_contrast.append(1)\n",
    "\n",
    "\n",
    "    second_level_input = file_list\n",
    "\n",
    "    design_columns = ['intercept','main_condition','main_PE','interaction']\n",
    "\n",
    "    X = pd.concat([pd.Series(np.ones(len(file_list))), \n",
    "                pd.Series(main_condition_contrast), \n",
    "                pd.Series(main_PE_contrast), \n",
    "                pd.Series(interaction_contrast)], axis=1)\n",
    "    X.columns = design_columns\n",
    "\n",
    "    second_level_input = file_list\n",
    "    second_level_model = SecondLevelModel(mask_img=gm_mask_img)\n",
    "    second_level_model = second_level_model.fit(second_level_input, design_matrix=X)\n",
    "    stats_name = 'effect_size'\n",
    "    \n",
    "    res = second_level_model.compute_contrast('interaction',output_type=stats_name)\n",
    "    nii_file_path = sub_output_dir + f'/interaction_{stats_name}.nii.gz'\n",
    "    nib.save(res, nii_file_path)\n",
    "\n",
    "    res = second_level_model.compute_contrast('main_PE',output_type=stats_name)\n",
    "    nii_file_path = sub_output_dir + f'/main_PE_{stats_name}.nii.gz'\n",
    "    nib.save(res, nii_file_path)\n",
    "\n",
    "    res = second_level_model.compute_contrast('main_condition',output_type=stats_name)\n",
    "    nii_file_path = sub_output_dir + f'/main_condition_{stats_name}.nii.gz'\n",
    "    nib.save(res, nii_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running subject 152\n",
      "running subject 179\n",
      "running subject 154\n",
      "running subject 158\n",
      "running subject 173\n",
      "running subject 153\n",
      "running subject 159\n",
      "running subject 174\n",
      "running subject 162\n",
      "running subject 145\n",
      "running subject 143\n",
      "running subject 181\n",
      "running subject 144\n",
      "running subject 169\n",
      "running subject 146\n",
      "running subject 167\n",
      "running subject 161\n",
      "running subject 182\n",
      "running subject 147\n",
      "running subject 166\n",
      "running subject 160\n",
      "running subject 185\n",
      "running subject 170\n",
      "running subject 176\n",
      "running subject 151\n",
      "running subject 157\n",
      "running subject 171\n",
      "running subject 177\n",
      "running subject 150\n",
      "running subject 156\n"
     ]
    }
   ],
   "source": [
    "for s in subjects_list:\n",
    "    sub_output_dir = second_level_res_dir + f'/{s}/'\n",
    "    # run contrast:\n",
    "    print(f'running subject {s}')   \n",
    "    social_PE_congruent_contrast, pattern_PE_congruent_contrast = [], []\n",
    "    social_PE_contruent_input, pattern_PE_contruent_input = [], []   \n",
    "    \n",
    "    file_name = glob.glob(beta_dir + f'{s}/sub-{s}*beta*fb_Social_PE*.nii.gz')\n",
    "    social_PE_contruent_input.append(file_name[0])\n",
    "    social_PE_congruent_contrast.append(1)\n",
    "\n",
    "    file_name = glob.glob(beta_dir + f'{s}/sub-{s}*beta*fb_Social_Congruent*.nii.gz')\n",
    "    social_PE_contruent_input.append(file_name[0])\n",
    "    social_PE_congruent_contrast.append(-1)\n",
    "\n",
    "    design_columns = ['intercept','fb_social_PE_congruent']\n",
    "\n",
    "    X = pd.concat([pd.Series(np.ones(len(social_PE_contruent_input))), \n",
    "                pd.Series(social_PE_congruent_contrast)], axis=1)\n",
    "    X.columns = design_columns\n",
    "\n",
    "    second_level_input = social_PE_contruent_input\n",
    "    second_level_model = SecondLevelModel(mask_img=gm_mask_img)\n",
    "    second_level_model = second_level_model.fit(second_level_input, design_matrix=X)\n",
    "\n",
    "    stats_name = 'effect_size'\n",
    "    res = second_level_model.compute_contrast('fb_social_PE_congruent',output_type=stats_name)\n",
    "\n",
    "    nii_file_path = sub_output_dir + f'/fb_social_PE_congruent_{stats_name}.nii.gz'\n",
    "    nib.save(res, nii_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running subject 152\n",
      "running subject 179\n",
      "running subject 154\n",
      "running subject 158\n",
      "running subject 173\n",
      "running subject 153\n",
      "running subject 159\n",
      "running subject 174\n",
      "running subject 162\n",
      "running subject 145\n",
      "running subject 143\n",
      "running subject 181\n",
      "running subject 144\n",
      "running subject 169\n",
      "running subject 146\n",
      "running subject 167\n",
      "running subject 161\n",
      "running subject 182\n",
      "running subject 147\n",
      "running subject 166\n",
      "running subject 160\n",
      "running subject 185\n",
      "running subject 170\n",
      "running subject 176\n",
      "running subject 151\n",
      "running subject 157\n",
      "running subject 171\n",
      "running subject 177\n",
      "running subject 150\n",
      "running subject 156\n"
     ]
    }
   ],
   "source": [
    "for s in subjects_list:\n",
    "    sub_output_dir = second_level_res_dir + f'/{s}/'\n",
    "    # run contrast:\n",
    "    print(f'running subject {s}')     \n",
    "    file_name = glob.glob(beta_dir + f'{s}/sub-{s}*beta*fb_Pattern_PE*.nii.gz')\n",
    "    pattern_PE_contruent_input.append(file_name[0])\n",
    "    pattern_PE_congruent_contrast.append(1)\n",
    "\n",
    "    file_name = glob.glob(beta_dir + f'{s}/sub-{s}*beta*fb_Pattern_Congruent*.nii.gz')\n",
    "    pattern_PE_contruent_input.append(file_name[0])\n",
    "    pattern_PE_congruent_contrast.append(-1)\n",
    "\n",
    "    design_columns = ['intercept','fb_pattern_PE_congruent']\n",
    "\n",
    "    X = pd.concat([pd.Series(np.ones(len(pattern_PE_contruent_input))), \n",
    "                pd.Series(pattern_PE_congruent_contrast)], axis=1)\n",
    "    X.columns = design_columns\n",
    "\n",
    "    second_level_input = pattern_PE_contruent_input\n",
    "    second_level_model = SecondLevelModel()\n",
    "    second_level_model = second_level_model.fit(second_level_input, design_matrix=X)\n",
    "\n",
    "    stats_name = 'effect_size'\n",
    "    res = second_level_model.compute_contrast('fb_pattern_PE_congruent',output_type=stats_name)\n",
    "    nii_file_path = sub_output_dir + f'/fb_pattern_PE_congruent_{stats_name}.nii.gz'\n",
    "    nib.save(res, nii_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running subject 152\n",
      "running subject 179\n",
      "running subject 154\n",
      "running subject 158\n",
      "running subject 173\n",
      "running subject 153\n",
      "running subject 159\n",
      "running subject 174\n",
      "running subject 162\n",
      "running subject 145\n",
      "running subject 143\n",
      "running subject 181\n",
      "running subject 144\n",
      "running subject 169\n",
      "running subject 146\n",
      "running subject 167\n",
      "running subject 161\n",
      "running subject 182\n",
      "running subject 147\n",
      "running subject 166\n",
      "running subject 160\n",
      "running subject 185\n",
      "running subject 170\n",
      "running subject 176\n",
      "running subject 151\n",
      "running subject 157\n",
      "running subject 171\n",
      "running subject 177\n",
      "running subject 150\n",
      "running subject 156\n"
     ]
    }
   ],
   "source": [
    "# fb_PE_congruent\n",
    "\n",
    "\n",
    "for s in subjects_list:\n",
    "    sub_output_dir = second_level_res_dir + f'/{s}/'\n",
    "    # run contrast:\n",
    "    print(f'running subject {s}')  \n",
    "\n",
    "    PE_congruent_input, PE_congruent_contrast = [], []\n",
    "       \n",
    "    file_name = glob.glob(beta_dir + f'{s}/sub-{s}*beta_video-fb_Pattern_PE*.nii.gz')\n",
    "    PE_congruent_input.append(file_name[0])\n",
    "    PE_congruent_contrast.append(1)\n",
    "\n",
    "    file_name = glob.glob(beta_dir + f'{s}/sub-{s}*beta_video-fb_Social_PE*.nii.gz')\n",
    "    PE_congruent_input.append(file_name[0])\n",
    "    PE_congruent_contrast.append(1)\n",
    "\n",
    "    file_name = glob.glob(beta_dir + f'{s}/sub-{s}*beta_video-fb_Pattern_Congruent*.nii.gz')\n",
    "    PE_congruent_input.append(file_name[0])\n",
    "    PE_congruent_contrast.append(-1)\n",
    "\n",
    "    file_name = glob.glob(beta_dir + f'{s}/sub-{s}*beta_video-fb_Social_Congruent*.nii.gz')\n",
    "    PE_congruent_input.append(file_name[0])\n",
    "    PE_congruent_contrast.append(-1)\n",
    "\n",
    "    design_columns = ['intercept','fb_PE_congruent']\n",
    "\n",
    "    X = pd.concat([pd.Series(np.ones(len(PE_congruent_input))), \n",
    "                pd.Series(PE_congruent_contrast)], axis=1)\n",
    "    X.columns = design_columns\n",
    "\n",
    "    second_level_input = PE_congruent_input\n",
    "    second_level_model = SecondLevelModel()\n",
    "    second_level_model = second_level_model.fit(second_level_input, design_matrix=X)\n",
    "\n",
    "    stats_name = 'effect_size'\n",
    "    res = second_level_model.compute_contrast('fb_PE_congruent',output_type=stats_name)\n",
    "    nii_file_path = sub_output_dir + f'/fb_PE_congruent_{stats_name}.nii.gz'\n",
    "    nib.save(res, nii_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running group level contrast: obs_social_pattern\n",
      "running group level contrast: interaction\n",
      "running group level contrast: main_PE\n",
      "running group level contrast: main_condition\n",
      "running group level contrast: fb_social_PE_congruent\n",
      "running group level contrast: fb_pattern_PE_congruent\n",
      "running group level contrast: fb_PE_congruent\n"
     ]
    }
   ],
   "source": [
    "# group level:\n",
    "\n",
    "contrast_list = ['obs_social_pattern', 'interaction', 'main_PE', 'main_condition', 'fb_social_PE_congruent', 'fb_pattern_PE_congruent', 'fb_PE_congruent']\n",
    "for contrast in contrast_list:\n",
    "    print(f'running group level contrast: {contrast}')\n",
    "    group_level_input = glob.glob(second_level_res_dir + f'/*/{contrast}_effect_size.nii.gz')\n",
    "\n",
    "    design_columns = ['intercept']\n",
    "    X = pd.DataFrame(np.ones(len(group_level_input)), columns = design_columns)\n",
    "\n",
    "    group_level_model = SecondLevelModel()\n",
    "    group_level_model = group_level_model.fit(group_level_input, design_matrix=X)\n",
    "    group_res = group_level_model.compute_contrast('intercept',output_type='all')\n",
    "    for stats_name in group_res.keys():\n",
    "        res = group_res[stats_name]\n",
    "        nii_file_path = second_level_res_dir + f'/{contrast}_{stats_name}.nii.gz'\n",
    "        nib.save(res, nii_file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contrast: obs_social_pattern\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yiyuwang/anaconda3/envs/neuroimaging_env/lib/python3.7/site-packages/nilearn/_utils/param_validation.py:73: UserWarning: The given float value must not exceed 4.730902651176177. But, you have given threshold=5.149072733780298.\n",
      "  f\"The given float value must not exceed {value_check}. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FWE corrected p value is 5.149072733780298\n",
      "max value is 4.730902651176177\n",
      "min value is -3.419279368772976\n",
      "contrast: interaction\n",
      "FWE corrected p value is 5.149072733780298\n",
      "max value is 6.092518757011305\n",
      "min value is -5.9509785491284575\n",
      "contrast: main_PE\n",
      "FWE corrected p value is 5.149072733780298\n",
      "max value is 4.941261744419585\n",
      "min value is -7.093916147487632\n",
      "contrast: main_condition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yiyuwang/anaconda3/envs/neuroimaging_env/lib/python3.7/site-packages/nilearn/_utils/param_validation.py:73: UserWarning: The given float value must not exceed 5.051749669722499. But, you have given threshold=5.149072733780298.\n",
      "  f\"The given float value must not exceed {value_check}. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FWE corrected p value is 5.149072733780298\n",
      "max value is 5.051749669722499\n",
      "min value is -3.5689180512265213\n",
      "contrast: fb_social_PE_congruent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yiyuwang/anaconda3/envs/neuroimaging_env/lib/python3.7/site-packages/nilearn/_utils/param_validation.py:73: UserWarning: The given float value must not exceed 5.076379033339101. But, you have given threshold=5.149072733780298.\n",
      "  f\"The given float value must not exceed {value_check}. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FWE corrected p value is 5.149072733780298\n",
      "max value is 4.683319935802479\n",
      "min value is -5.076379033339101\n",
      "contrast: fb_pattern_PE_congruent\n",
      "FWE corrected p value is 5.149072733780298\n",
      "max value is 11.908241687996963\n",
      "min value is -10.958846449543787\n",
      "contrast: fb_PE_congruent\n",
      "FWE corrected p value is 5.149072733780298\n",
      "max value is 3.517386066663037\n",
      "min value is -3.7917926876932437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yiyuwang/anaconda3/envs/neuroimaging_env/lib/python3.7/site-packages/nilearn/_utils/param_validation.py:73: UserWarning: The given float value must not exceed 3.7917926876932437. But, you have given threshold=5.149072733780298.\n",
      "  f\"The given float value must not exceed {value_check}. \"\n"
     ]
    }
   ],
   "source": [
    "for contrast in contrast_list:\n",
    "    print(f'contrast: {contrast}')\n",
    "    z_map = nib.load(second_level_res_dir + f'/{contrast}_z_score.nii.gz')\n",
    "    fwe_corrected_img, fwe = threshold_stats_img(z_map, mask_img=gm_mask_img, alpha=0.05, cluster_threshold = 50, height_control='bonferroni')\n",
    "    print(f'FWE corrected p value is {fwe}')\n",
    "    print(f'max value is {np.max(z_map.get_fdata())}')\n",
    "    print(f'min value is {np.min(z_map.get_fdata())}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
